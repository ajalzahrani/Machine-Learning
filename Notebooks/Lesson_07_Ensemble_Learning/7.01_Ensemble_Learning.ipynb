{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"pPi899u95iC2"},"source":["### <b> Ensemble Learning </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4IDGoFSU5nXb"},"source":["### <b> Learning Objectives </b>\n","By the end of this lesson, you will be able to:\n","- Define ensemble learning\n","- List different types of ensemble methods\n","- Build an intuition\n","- Apply different algorithms of ensemble learning using use cases"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wtzozj625qOm"},"source":["### <b> What Is Ensemble Learning? </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Td7-rOBj5usF"},"source":["Ensemble techniques combine individual models to improve the stability and predictive power of the model."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wvVZB0oo54Ou"},"source":["#### <b> Ideology Behind Ensemble Learning: </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Uvb55H6-57G_"},"source":["* Certain models do well in modeling one aspect of the data, while others do well in modeling another.\n","\n","* Instead of learning a single complex model, learn several simple models and combine their output to produce the final decision.\n","\n","* Individual model variances and biases are balanced by the strength of other models in ensemble learning.\n","\n","* Ensemble learning will provide a composite prediction where the final accuracy is better than the accuracy of individual models."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WMhoybU359s1"},"source":["#### <b> Working of Ensemble Learning </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LBaVCR3B6AOG"},"source":["![Ensemble_Learning_Workflow](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/Ensemble_Learning_Workflow.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6KwXIaUo6E6e"},"source":["#### <b> Significance of Ensemble Learning </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ml1ie17T6HcP"},"source":["* Robustness\n","  - Ensemble models incorporate the predictions from all the base learners\n","* Accuracy\n","  - Ensemble models deliver accurate predictions and have improved performances"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VavpTzv26ON4"},"source":["#### <b> Ensemble Learning Methods </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"S151Fz-r6STd"},"source":["* Techniques for creating an ensemble model\n","* Combine all weak learners to form an ensemble, or create an ensemble of well-chosen strong and diverse models"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Jt9iwlMW6VPp"},"source":["#### <b> Steps Involved in Ensemble Methods </b>\n","\n","Every ensemble algorithm consists of two steps:\n","\n","* Producing a cohort of predictions using simple ML algorithms\n","* Combining the predictions into one aggregated model\n","\n","The ensemble can be achieved through several techniques."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Dfc4kJRf6Y8d"},"source":["### <b> Types of Ensemble Methods </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pDRgzaPE6duh"},"source":["#### <b> Averaging </b>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DisGQkqI6gZP"},"source":["![Averaging](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/Averaging.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"75i3XpeO6lmL"},"source":["#### <b> Weighted Averaging </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cHaLDPhL6zM-"},"source":["![Weighted_Averaging](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/Weighted_Averaging.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DGm8JLhT663p"},"source":["### <b> Bagging Algorithms </b>\n","\n","Bootstrap Aggregation or bagging involves taking multiple samples from your training dataset (with replacement) and training a model for each sample.\n","\n","The final output prediction is averaged across the predictions of all of the submodels.\n","\n","The three bagging models covered in this section are as follows:\n","\n"," - Bagged Decision Trees\n"," - Random Forest\n"," - Extra Trees"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"COHCIquy7NOf"},"source":["#### <b> 1. Bagged Decision Trees </b>\n","\n","Bagging performs best with algorithms that have a high variance. A popular example is decision trees, often constructed without pruning.\n","\n","Below, you can see an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xx8xgGfceF5u"},"source":["- Scikit-learn is a Python library that provides a consistent interface for machine learning and statistical modeling, including classification, regression, clustering, and dimensionality reduction.\n","- Pandas is a Python library for data manipulation and analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJuR-17J7QYl","outputId":"e6ab3d1b-1352-4e0e-edc8-86166269805d"},"outputs":[],"source":["#Bagged Decision Trees for Classification\n","import pandas as pd\n","import requests,io\n","from sklearn import model_selection\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","import ssl\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","x = requests.get(url=url).content \n","dataframe = pd.read_csv(io.StringIO(x.decode('utf8')),names=names)\n","#dataframe = pandas.read_csv(url, names=names)\n","array = dataframe.values\n","\n","X = array[:,0:8]\n","Y = array[:,8]\n","\n","seed = 7\n","\n","kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n","cart = DecisionTreeClassifier()\n","num_trees = 100\n","\n","model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n","\n","results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Swj2Rvoa7IZF"},"source":["#### <b> 2. Random Forest </b> \n","\n","Random forest is an extension of bagged decision trees.\n","\n","Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features is considered for each split.\n","\n","You can construct a Random Forest model for classification using the RandomForestClassifier class.\n","\n","The example below provides a sample of Random Forest for classification with 100 trees and split points chosen from a random selection of three features.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odZ4P_P37Uis","outputId":"a180e4c4-1701-4366-aade-638770428d54"},"outputs":[],"source":["#Random Forest Classification\n","import pandas\n","from sklearn import model_selection\n","from sklearn.ensemble import RandomForestClassifier\n","\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = pandas.read_csv(url, names=names)\n","array = dataframe.values\n","\n","X = array[:,0:8]\n","Y = array[:,8]\n","seed = 7\n","num_trees = 100\n","max_features = 3\n","\n","kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n","model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n","results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FqyHO74A7ZOt"},"source":["#### <b> 3. Extra Trees</b>\n","\n","Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.\n","\n","You can construct an Extra Trees model for classification using the ExtraTreesClassifier class.\n","\n","The example below provides a demonstration of extra trees with a tree set of 100 and splits chosen from seven random features.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkf_0b5l7cq9","outputId":"47fe251e-ffaa-4080-d018-24c0fdd87921"},"outputs":[],"source":["#Extra Trees Classification\n","import pandas\n","from sklearn import model_selection\n","from sklearn.ensemble import ExtraTreesClassifier\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = pandas.read_csv(url, names=names)\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","seed = 7\n","num_trees = 100\n","max_features = 7\n","kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n","model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n","results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jlIFvFor7hmR"},"source":["###  <b> Boosting Algorithms </b>\n","\n","Boosting ensemble algorithms create a sequence of models that attempts to correct the mistakes of the models before them in the sequence.\n","\n","Once created, the models make predictions that may be weighted by their demonstrated accuracy, and the results are combined to create a final output prediction.\n","\n","\n","The two most common boosting ensemble machine learning algorithms are:\n","\n","- AdaBoost\n","\n","- Stochastic Gradient Boosting\n","<br>\n","\n","#### <b> AdaBoost </b>\n","\n","AdaBoost was the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay more or less attention to them in the construction of subsequent models.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7aO-zHHTU1mN"},"source":["![AdaBoost](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/AdaBoost.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QHCG5BI47oIg"},"source":["You can construct an AdaBoost model for classification using the AdaBoostClassifier class.\n","\n","The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmfduWVV7qeq","outputId":"4817bcc8-3698-4195-e7c9-1a3bebac8eb6"},"outputs":[],"source":["#AdaBoost Classification\n","import pandas\n","from sklearn import model_selection\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","\n","dataframe = pandas.read_csv(url, names=names)\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","\n","seed = 7\n","num_trees = 30\n","\n","kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n","model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n","\n","results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E2fRqjMx7tQk"},"source":["#### <b> Stochastic Gradient Boosting </b>\n","\n","One of the most advanced ensemble approaches is Stochastic Gradient Boosting (also known as Gradient Boosting Machines). It's also a strategy that's proven to be one of the most effective methods for boosting performance via ensemble."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"J5tYi9Q37wWu"},"source":["#### <b> Steps of Gradient Boasting Machine </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vTeY0JU97zvU"},"source":["![GBM_Steps](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/GBM_Steps.PNG)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ilE5qgrG7_FP"},"source":["You can construct a Gradient Boosting model for classification using the **GradientBoostingClassifier** class.\n","\n","The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCYRA8lH8BQx","outputId":"fc19a604-3b82-4b22-a3b0-e929bf7472e6"},"outputs":[],"source":["#Stochastic Gradient Boosting Classification\n","import pandas\n","from sklearn import model_selection\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = pandas.read_csv(url, names=names)\n","\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","seed = 7\n","num_trees = 100\n","kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n","model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n","results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3ncdJlfL8EyT"},"source":["### CatBoost\n","\n","CatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers and is used for search, recommendation systems, personal assistants, self-driving cars, weather prediction, and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is open-source and can be used by anyone.\n","\n","Let's study this with the help of a use case.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NfzCTxdM9tz2"},"source":["#### <b> Data Description </b>\n","The data consists of real historical data collected from 2010 & 2011. Employees are manually allowed or denied access to resources over time. You must create an algorithm capable of learning from this historical data to predict approval or denial for an unknown set of employees.\n","\n","#### <b> File Descriptions </b>\n","\n","**train.csv:** It is a training set. Each row has the action (ground truth), resources, and information about the employee's role at the time of approval.\n","\n","**test.csv:** It is the test set for which predictions should be made. Each row asks whether an employee having the listed characteristics should have access to the listed resource.\n","\n","The objective is to develop a model from historical data that will decide the access needs of an employee so that manual access transactions (grants and revocations) are reduced as the attributes of the employee change over time. The model will take information on the position of an employee and a resource code and return whether access should be given or not.\n","\n","Note: The problem statement is from a Kaggle contest"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FiB_jNkd-PNx"},"source":["The objective is to develop a model from historical data, that will decide the access needs of an employee, so that manual access transactions (grants and revocations) are reduced as the attributes of the employee change over time. The model will take information on the position of an employee and a resource code and return whether access should be given or not. <br>\n","`Note: The problem statement is from a Kaggle contest`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"O2Nm6BAT5OdY"},"source":["#### <b> Libraries Installation </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al4b3_2kShpV"},"outputs":[],"source":["#Installing CatBoost\n","!pip3.10 install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTMJOp4Z5Odk","outputId":"f89ad23e-3d6a-4c32-f148-9410ccb9b836"},"outputs":[],"source":["#To import libraries\n","import catboost\n","print(catboost.__version__)\n","!python --version"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9IvIcY_05Odt"},"source":["#### <b> Reading the Data </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bazUWz1i5Odu"},"outputs":[],"source":["#To read the data\n","import pandas as pd\n","import os\n","import numpy as np\n","np.set_printoptions(precision=4)\n","import catboost\n","from catboost import *\n","from catboost import datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHhZNN1K5Od2"},"outputs":[],"source":["(train_df, test_df) = catboost.datasets.amazon()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdHqNPC25Od8","outputId":"defc4746-3a75-4dfb-86a8-830186de05bf"},"outputs":[],"source":["train_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2cpBrqOFShpW"},"source":["The data will be displayed on the screen."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-qQrg9yc5OeC"},"source":["#### <b> Preparing Your Data </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fP5OC4_G5OeF"},"source":["Label values extraction"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qJJ7w958Nklf"},"source":["Action column contains the categorical feature. However, it is not available for test dataset, so you must drop the Action column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTjy52n15OeG"},"outputs":[],"source":["y = train_df.ACTION\n","X = train_df.drop('ACTION', axis=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"do17No295OeO"},"source":["Categorical features declaration \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"O59buCb9Nklf"},"source":["* <b>cat_features </b> is a one-dimensional array of categorical columns indices. \n","* It has one of the following types: list, numpy.ndarray, pandas.DataFrame, and pandas.Series."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zV5KB_szNklg"},"source":["Now we will declare the cat feature that holds the categorical values present on train dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mq03gQnU5OeP","outputId":"acf6c36c-1f8c-479a-e704-095720ab2299"},"outputs":[],"source":["#The type list is used here\n","cat_features = list(range(0, X.shape[1]))\n","print(cat_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYryzS3m5OeY","outputId":"34a7ff68-8b88-47b9-f288-2d802d5d55e4"},"outputs":[],"source":["#looking for label balance in dataset\n","print('Labels: {}'.format(set(y)))\n","print('Zero count = {}, One count = {}'.format(len(y) - sum(y), sum(y)))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2lZni3Qb5Oed"},"source":["Ways to create **Pool** class\n","- In multiprocessing, the Pool class may handle a huge number of processes. It enables you to run several jobs in a single process due to its ability to queue the jobs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edN1u7wx5Oef"},"outputs":[],"source":["#Specifying the dataset\n","dataset_dir = './amazon'\n","if not os.path.exists(dataset_dir):\n","    os.makedirs(dataset_dir)\n","\n","#We will be able to work with files with/without header and with different separators\n","train_df.to_csv(\n","    os.path.join(dataset_dir, 'train.tsv'),\n","    index=False, sep='\\t', header=False\n",")\n","test_df.to_csv(\n","    os.path.join(dataset_dir, 'test.tsv'),\n","    index=False, sep='\\t', header=False\n",")\n","\n","train_df.to_csv(\n","    os.path.join(dataset_dir, 'train.csv'),\n","    index=False, sep=',', header=True\n",")\n","test_df.to_csv(\n","    os.path.join(dataset_dir, 'test.csv'),\n","    index=False, sep=',', header=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8pHZhnF5Oel","outputId":"1df49bb2-c5b4-4add-b186-136145afe095"},"outputs":[],"source":["!head amazon/train.csv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DBuSqjQ5Oer"},"outputs":[],"source":["from catboost.utils import create_cd\n","feature_names = dict()\n","for column, name in enumerate(train_df):\n","    if column == 0:\n","        continue\n","    feature_names[column - 1] = name\n","    \n","create_cd(\n","    label=0, \n","    cat_features=list(range(1, train_df.columns.shape[0])),\n","   \n","    output_path=os.path.join(dataset_dir, 'train.cd')\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9NOIq_F5Oex","outputId":"fb865df9-05c6-44dc-db31-d346943a0689"},"outputs":[],"source":["!cat amazon/train.cd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9P75ddI5Oe4","outputId":"f8953e1c-c097-4b7a-a7fa-98897916effa"},"outputs":[],"source":["pool1 = Pool(data=X, label=y, cat_features=cat_features)\n","pool2 = Pool(\n","    data=os.path.join(dataset_dir, 'train.csv'), \n","    delimiter=',', \n","    column_description=os.path.join(dataset_dir, 'train.cd'),\n","    has_header=True\n",")\n","pool3 = Pool(data=X, cat_features=cat_features)\n","\n","#Fastest way to create a Pool is to create it from numpy matrix.\n","#This way should be used if you want fast predictions\n","#or fastest way to load the data in python.\n","\n","X_prepared = X.values.astype(str).astype(object)\n","#For FeaturesData class categorial features must have type str\n","\n","pool4 = Pool(\n","    data=FeaturesData(\n","        cat_feature_data=X_prepared,\n","        cat_feature_names=list(X)\n","    ),\n","    label=y.values\n",")\n","\n","print('Dataset shape')\n","print('dataset 1:' + str(pool1.shape) +\n","      '\\ndataset 2:' + str(pool2.shape) + \n","      '\\ndataset 3:' + str(pool3.shape) +\n","      '\\ndataset 4: ' + str(pool4.shape))\n","\n","print('\\n')\n","print('Column names')\n","print('dataset 1:')\n","print(pool1.get_feature_names()) \n","print('\\ndataset 2:')\n","print(pool2.get_feature_names())\n","print('\\ndataset 3:')\n","print(pool3.get_feature_names())\n","print('\\ndataset 4:')\n","print(pool4.get_feature_names())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fXtl9SJU5Oe-"},"source":["#### <b> Split Your Data into Train and Validation </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HQEc29rVShpY"},"source":["Let us split the data into **Train** and **Validation**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cE-9zYl65Oe_"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8, random_state=1234)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Pmy68aJN5OfF"},"source":["#### <b> Selecting the Objective Function </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6pmmHAOk5OfG"},"source":["Possible options for binary classification:\n","\n","`Logloss`\n","\n","`CrossEntropy` for probabilities in target"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iNvDzSNatd-r"},"source":["A **CatBoostClassifier** trains and applies models for the classification problems. It provides compatibility with the scikit-learn tools."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKW4qD2V5OfI","outputId":"799de3ac-0f50-4d9f-9a44-66718231b5d3"},"outputs":[],"source":["from catboost import CatBoostClassifier\n","model = CatBoostClassifier(\n","    iterations=5,\n","    learning_rate=0.1,\n","    #loss_function='CrossEntropy'\n",")\n","model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n","    verbose=False\n",")\n","print('Model is fitted: ' + str(model.is_fitted()))\n","print('Model params:')\n","print(model.get_params())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZzrnZlaS5OfP"},"source":["#### <b> Stdout of the Training </b>\n","Stdout displays output directly to the screen console.\n","Output can take any form.\n","It can be output from a print statement, an expression statement, or even a direct prompt.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlkrU9zN5OfQ","outputId":"07632a8e-ee88-4db0-e8e7-a782dac1b610"},"outputs":[],"source":["from catboost import CatBoostClassifier\n","model = CatBoostClassifier(\n","    iterations=15,\n","#verbose=5,\n",")\n","model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zRUBDxgk5OfX"},"source":["#### <b> Metric Calculation and Graph Plotting </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bxX9-FWZShpZ"},"source":["Let us perform metric calculation and graph plotting by importing the **CatBoostClassifier**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["e62d7e59b8e54eb984d9ff6952a21d36","d4a874c1c53a4e838f4c23ac98f85d24"]},"id":"nq9gp0YE5OfY","outputId":"eca40a54-b8ce-4e6c-8b5f-508305047eb5"},"outputs":[],"source":["from catboost import CatBoostClassifier\n","model = CatBoostClassifier(\n","    iterations=50,\n","    random_seed=63,\n","    learning_rate=0.5,\n","    custom_loss=['AUC', 'Accuracy']\n",")\n","model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n","    verbose=False,\n","    plot=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DemIlngc5Ofd"},"source":["#### <b> Model Comparison </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qfLm729MShpZ"},"source":["Let us compare the models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oC9hegPs5Ofe","outputId":"73bbe9cf-efba-4020-ebd2-a6fd8fff56d5"},"outputs":[],"source":["model1 = CatBoostClassifier(\n","    learning_rate=0.7,\n","    iterations=100,\n","    random_seed=0,\n","    train_dir='learing_rate_0.7'\n",")\n","\n","model2 = CatBoostClassifier(\n","    learning_rate=0.01,\n","    iterations=100,\n","    random_seed=0,\n","    train_dir='learing_rate_0.01'\n",")\n","model1.fit(\n","    X_train, y_train,\n","    eval_set=(X_validation, y_validation),\n","    cat_features=cat_features,\n","    verbose=False\n",")\n","model2.fit(\n","    X_train, y_train,\n","    eval_set=(X_validation, y_validation),\n","    cat_features=cat_features,\n","    verbose=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["b8aacfcf2d224c00b22a31bee8287cf0","82f6f10a397b468f8acb7e26d289ed49"]},"id":"_eFSknNm5Ofi","outputId":"2f4d1802-1b08-46ac-d59f-571e7c1cee1e"},"outputs":[],"source":["from catboost import MetricVisualizer\n","MetricVisualizer(['learing_rate_0.01', 'learing_rate_0.7']).start()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bbW98AMw5Ofm"},"source":["#### <b> Best Iteration </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["91d7354aa35844bfab82f40bf08d2fc0","a0a59673a8cb47ec95ee513789c0a61c"]},"id":"ZkfYD0ht5Ofn","outputId":"cc6148f2-a825-4e54-cec0-09f1374ec479"},"outputs":[],"source":["#Performing best iteration\n","from catboost import CatBoostClassifier\n","model = CatBoostClassifier(\n","    iterations=100,\n","    random_seed=63,\n","    learning_rate=0.5,\n","#use_best_model=False\n",")\n","model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n","    verbose=False,\n","    plot=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPJ8Uy7o5Ofs","outputId":"81d939e9-cd36-4511-b750-c9397b3a86ea"},"outputs":[],"source":["print('Tree count: ' + str(model.tree_count_))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RcUeZopH5Ofw"},"source":["#### <b> Cross-Validation </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DX_ow3UNNklj"},"source":["Cross-validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model.\n","CatBoost allows to perform cross-validation on the given dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["76bc366fdb7c40ac9c95a1210ff894de","3ff3b0e279cd4b399718e8947d61ceb1"]},"id":"YpzDR3Qn5Ofx","outputId":"75a3060f-94ab-404c-9a6f-0d70acfc0f09"},"outputs":[],"source":["#Performing cross-validation\n","from catboost import cv\n","\n","params = {}\n","params['loss_function'] = 'Logloss'\n","params['iterations'] = 80\n","params['custom_loss'] = 'AUC'\n","params['random_seed'] = 63\n","params['learning_rate'] = 0.5\n","\n","cv_data = cv(\n","    params = params,\n","    pool = Pool(X, label=y, cat_features=cat_features),\n","    fold_count=4,\n","    shuffle=True,\n","    partition_random_seed=0,\n","    plot=True,\n","    stratified=False,\n","    verbose=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bZHjNBk5Of2","outputId":"6977177c-3046-466c-8a46-d01c1331f404"},"outputs":[],"source":["cv_data.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3onuoT6BShpb"},"source":["<b> Logloss </b> is indicative of how close the prediction probability is to the corresponding true value.\n","\n","Let us print the **Best validation Logloss score**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3G_wLwaC5Of5","outputId":"73c6a6fb-e8be-4fa3-d69f-7c356489ee7b"},"outputs":[],"source":["best_value = np.min(cv_data['test-Logloss-mean'])\n","best_iter = np.argmin(cv_data['test-Logloss-mean'])\n","\n","print('Best validation Logloss score, not stratified: {:.4f}±{:.4f} on step {}'.format(\n","    best_value,\n","    cv_data['test-Logloss-std'][best_iter],\n","    best_iter)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["f33d77a327b240e0a6f7ec8ef1b14605","b5b4aa4b383e46e998644f18996bced8"]},"id":"cmZMkPh15OgC","outputId":"ecf5ece0-f367-4616-eaba-984793bc3238"},"outputs":[],"source":["cv_data = cv(\n","    params = params,\n","    pool = Pool(X, label=y, cat_features=cat_features),\n","    fold_count=5,\n","    type = 'Classical',\n","    shuffle=True,\n","    partition_random_seed=0,\n","    plot=True,\n","    stratified=True,\n","    verbose=False\n",")\n","\n","best_value = np.min(cv_data['test-Logloss-mean'])\n","best_iter = np.argmin(cv_data['test-Logloss-mean'])\n","\n","print('Best validation Logloss score, stratified: {:.4f}±{:.4f} on step {}'.format(\n","    best_value,\n","    cv_data['test-Logloss-std'][best_iter],\n","    best_iter)\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4G_qISWi5OgH"},"source":["####  <b> Overfitting Detector </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"D8wK2NxxNklk"},"source":["If overfitting occurs, CatBoost can stop the training earlier than the training parameters dictate. For example, it can be stopped before the specified number of trees are built. This option is set in the starting parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["0e1841ffcc2948829815023ddef4adf2","9eec2a0b54904c8b88a573d249aab21a"]},"id":"90aASIdT5OgH","outputId":"197b26e6-c680-4644-dfa0-1829d4e59714"},"outputs":[],"source":["model_with_early_stop = CatBoostClassifier(\n","    iterations=200,\n","    random_seed=63,\n","    learning_rate=0.5,\n","    early_stopping_rounds=20\n",")\n","model_with_early_stop.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n","    verbose=False,\n","    plot=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kE8PeJo75OgM","outputId":"5563415b-f409-4072-cbd9-39cc2daa08a0"},"outputs":[],"source":["print(model_with_early_stop.tree_count_)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["1531eed4e779494583a0afb80ba0c36c","7a7827852e6e4d4dae09925aa203ed7a"]},"id":"krNSogvR5OgQ","outputId":"19b41933-621e-4ac6-c34a-d43e9e52a757"},"outputs":[],"source":["model_with_early_stop = CatBoostClassifier(\n","    eval_metric='AUC',\n","    iterations=200,\n","    random_seed=63,\n","    learning_rate=0.5,\n","    early_stopping_rounds=20\n",")\n","model_with_early_stop.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n","    verbose=False,\n","    plot=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxBiKSFD5OgV","outputId":"5eceb682-e3d2-49bf-cb99-39926f9d86d2"},"outputs":[],"source":["print(model_with_early_stop.tree_count_)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PxAb9Kl15OgY"},"source":["#### <b> Select Decision Boundary </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KGCB5MV-Nklk"},"source":["In classification problems with two or more classes, a decision boundary is a hypersurface that separates the underlying vector space into sets, keeping one for each class."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["ff8a649adf424cbdb096e92ad9df871b","a78e54693e1148d4960144fdbf0ffe97"]},"id":"M1XlJ4ET5OgZ","outputId":"d300cb7a-9739-437c-bc33-c50457537cc1"},"outputs":[],"source":["model = CatBoostClassifier(\n","    random_seed=63,\n","    iterations=200,\n","    learning_rate=0.03,\n",")\n","model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    verbose=False,\n","    plot=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9gy--FHg5Ogd"},"source":["![Decision_Boundary](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/Decision_Boundary.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOwXIxd_5Oge"},"outputs":[],"source":["#Using utils to make the pattern easier\n","from catboost.utils import get_roc_curve\n","import sklearn\n","from sklearn import metrics\n","\n","eval_pool = Pool(X_validation, y_validation, cat_features=cat_features)\n","curve = get_roc_curve(model, eval_pool)\n","(fpr, tpr, thresholds) = curve\n","roc_auc = sklearn.metrics.auc(fpr, tpr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2D0OXjqK5Ogh","outputId":"ed129b7f-e51d-4217-f8bf-2958a6f88db4"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(16, 8))\n","lw = 2\n","\n","plt.plot(fpr, tpr, color='darkorange',\n","         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.5)\n","\n","plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', alpha=0.5)\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xticks(fontsize=16)\n","plt.yticks(fontsize=16)\n","plt.grid(True)\n","plt.xlabel('False Positive Rate', fontsize=16)\n","plt.ylabel('True Positive Rate', fontsize=16)\n","plt.title('Receiver operating characteristic', fontsize=20)\n","plt.legend(loc=\"lower right\", fontsize=16)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pm5u4WQyNkll"},"source":["The above graph illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6XnzMnU5Ogk"},"outputs":[],"source":["from catboost.utils import get_fpr_curve\n","from catboost.utils import get_fnr_curve\n","\n","(thresholds, fpr) = get_fpr_curve(curve=curve)\n","(thresholds, fnr) = get_fnr_curve(curve=curve)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yO22sy2R5Ogn","outputId":"29f539e2-7458-4192-fb96-51d2af58410a"},"outputs":[],"source":["plt.figure(figsize=(16, 8))\n","lw = 2\n","\n","plt.plot(thresholds, fpr, color='blue', lw=lw, label='FPR', alpha=0.5)\n","plt.plot(thresholds, fnr, color='green', lw=lw, label='FNR', alpha=0.5)\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xticks(fontsize=16)\n","plt.yticks(fontsize=16)\n","plt.grid(True)\n","plt.xlabel('Threshold', fontsize=16)\n","plt.ylabel('Error Rate', fontsize=16)\n","plt.title('FPR-FNR curves', fontsize=20)\n","plt.legend(loc=\"lower left\", fontsize=16)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T8lJSZAONkll"},"source":["The above graph displays the FPR-FNR curves for error rate and threshold."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAcclDiN5Ogr","outputId":"833b3f18-c1af-42b5-f1ad-e38bbae8bf29"},"outputs":[],"source":["from catboost.utils import select_threshold\n","\n","print(select_threshold(model=model, data=eval_pool, FNR=0.01))\n","print(select_threshold(model=model, data=eval_pool, FPR=0.01))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aIAjbgbE5Ogv"},"source":["#### <b> Snapshotting </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0RYEUiDVNkll"},"source":["Catboost supports snapshotting. You can use it to recover training after an interruption or start training with previous results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxB4pDMC5Ogw","outputId":"c635e351-1147-409e-d1f3-040be0cbae80","scrolled":true},"outputs":[],"source":["#!rm 'catboost_info/snapshot.bkp'\n","from catboost import CatBoostClassifier\n","model = CatBoostClassifier(\n","    iterations=100,\n","    save_snapshot=True,\n","    snapshot_file='snapshot.bkp',\n","    snapshot_interval=1,\n","    random_seed=43\n",")\n","model.fit(\n","    X_train, y_train,\n","    eval_set=(X_validation, y_validation),\n","    cat_features=cat_features,\n","    verbose=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vcDF3uub5Og0"},"source":["#### <b> Model Predictions </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Cw6A0o-fNklm"},"source":["predict_proba gives you the probabilities for the target in array form. The number of probabilities for each row is equal to the number of categories in the target variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZ-EOgNs5Og1","outputId":"7d214afd-e1df-42f0-d71f-7e7c76cffb31"},"outputs":[],"source":["print(model.predict_proba(X=X_validation))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QofNHstF5Og8","outputId":"5f26f440-f5a4-49c9-e265-366db4f00553"},"outputs":[],"source":["print(model.predict(data=X_validation))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wRvj9K45OhA","outputId":"05602437-9a51-40b4-9a64-eb247af6059d"},"outputs":[],"source":["raw_pred = model.predict(\n","    data=X_validation,\n","    prediction_type='RawFormulaVal'\n",")\n","print(raw_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPnLlKoP5OhF","outputId":"2e208fb7-ee97-4e99-eae4-777bd324ea52"},"outputs":[],"source":["from numpy import exp\n","\n","#Calculating sigmoid\n","sigmoid = lambda x: 1 / (1 + exp(-x))\n","\n","probabilities = sigmoid(raw_pred)\n","\n","print(probabilities)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tfu959oGNklm"},"source":["The probabilities will be displayed on the screen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOnzpfW_5OhJ","outputId":"0eeaa872-a1ee-4969-c6f4-1c6990c7a95e"},"outputs":[],"source":["X_prepared = X_validation.values.astype(str).astype(object)\n","#For FeaturesData class categorial features must have type str\n","\n","fast_predictions = model.predict_proba(\n","    X=FeaturesData(\n","        cat_feature_data=X_prepared,\n","        cat_feature_names=list(X_validation)\n","    )\n",")\n","print(fast_predictions)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j7nBYBzI5OhN"},"source":["#### <b> Staged Prediction </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vgJCOks8Nklm"},"source":["CatBoost allows to apply a trained model and calculate the results for each i-th tree of the model, taking into consideration only the trees in the range [0; i)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRYfQdbD5OhO","outputId":"af7c4b71-c11c-4486-db40-15ace5c03912"},"outputs":[],"source":["predictions_gen = model.staged_predict_proba(\n","    data=X_validation,\n","    ntree_start=0, \n","    ntree_end=5, \n","    eval_period=1\n",")\n","try:\n","    for iteration, predictions in enumerate(predictions_gen):\n","        print('Iteration ' + str(iteration) + ', predictions:')\n","        print(predictions)\n","except Exception:\n","    pass"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3aW6hlAi5OhR"},"source":["#### <b> Solving Multiclass Classification Problem </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rCwesf-eShpf"},"source":["Let us solve the **Multiclass Classification Problem** using the **CatBoostClassifier.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["0327b93beef24d888c8d7f0010219b84","22b5a0cdd5e649d9b35a4a039fb52bd1"]},"id":"rT5s48co5OhS","outputId":"da0eaf76-4345-4737-ac9c-bd8f5a788b5e"},"outputs":[],"source":["from catboost import CatBoostClassifier\n","model = CatBoostClassifier(\n","    iterations=50,\n","    random_seed=43,\n","    loss_function='MultiClass'\n",")\n","model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n","    verbose=False,\n","    plot=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FlpFRZCz5OhX"},"source":["For multiclass problems with many classes, sometimes, it's better to solve classification problems using ranking. To do that, we will build a dataset with groups. Every group will represent one object from our initial dataset. But it will have one additional categorical feature, a possible class value. Target values will be equal to 1 if the class value is equal to the correct class and 0 otherwise. Thus, each group will have exactly one 1 in labels and some zeros. You can put all possible class values in the group, or you can try setting only hard negatives if there are too many labels. We'll show this approach as an example of a binary classification problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIvkXuVl5OhX"},"outputs":[],"source":["#Defining custom function to build multiclass ranking\n","from copy import deepcopy\n","def build_multiclass_ranking_dataset(X, y, cat_features, label_values=[0,1], start_group_id=0):\n","    ranking_matrix = []\n","    ranking_labels = []\n","    group_ids = []\n","\n","    X_train_matrix = X.values\n","    y_train_vector = y.values\n","\n","    for obj_idx in range(X.shape[0]):\n","        obj = list(X_train_matrix[obj_idx])\n","\n","        for label in label_values:\n","            obj_of_given_class = deepcopy(obj)\n","            obj_of_given_class.append(label)\n","            ranking_matrix.append(obj_of_given_class)\n","            ranking_labels.append(float(y_train_vector[obj_idx] == label)) \n","            group_ids.append(start_group_id + obj_idx)\n","        \n","    final_cat_features = deepcopy(cat_features)\n","    final_cat_features.append(X.shape[1]) # new feature that we are adding should be categorical.\n","    return Pool(ranking_matrix, ranking_labels, cat_features=final_cat_features, group_id = group_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["4e0e1252d5b04dbca3952276a8475fd5","52b87a3ce3684f7e958ab1df8380d9d7"]},"id":"VlxBiDwl5Ohb","outputId":"4214d974-54bd-4c64-ec62-aed3acb1bfbd"},"outputs":[],"source":["from catboost import CatBoost\n","params = {'iterations':150, 'learning_rate':0.01, 'l2_leaf_reg':30, 'random_seed':0, 'loss_function':'QuerySoftMax'}\n","\n","groupwise_train_pool = build_multiclass_ranking_dataset(X_train, y_train, cat_features, [0,1])\n","groupwise_eval_pool = build_multiclass_ranking_dataset(X_validation, y_validation, cat_features, [0,1], X_train.shape[0])\n","\n","model = CatBoost(params)\n","model.fit(\n","    X=groupwise_train_pool,\n","    verbose=False,\n","    eval_set=groupwise_eval_pool,\n","    plot=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"e_jOxVQC5Ohm"},"source":["Making predictions with ranking mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LJ1_59F5Ohn","outputId":"29213586-7da0-4ad2-b4b5-efa2b2599ff7"},"outputs":[],"source":["import math\n","\n","obj = list(X_validation.values[0])\n","ratings = []\n","for label in [0,1]:\n","    obj_with_label = deepcopy(obj)\n","    obj_with_label.append(label)\n","    rating = model.predict([obj_with_label])[0]\n","    ratings.append(rating)\n","print('Raw values:', np.array(ratings))\n","\n","def soft_max(values):\n","    return [math.exp(val) / sum([math.exp(val) for val in values]) for val in values]\n","\n","print('Probabilities', np.array(soft_max(ratings)))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Cq-I_WzD5Ohq"},"source":["#### <b> Metric Evaluation on a New Dataset </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8N4RMYRjShpg"},"source":["Let us perform **Metric Evaluation** on a new dataset using the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFDHHfJi5Ohr","outputId":"21b31825-976c-4bcc-86f4-998030456ac9"},"outputs":[],"source":["model = CatBoostClassifier(\n","    random_seed=63,\n","    iterations=200,\n","    learning_rate=0.03,\n",")\n","model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    verbose=50\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["d6422b8f754f4f00a51745e654eae249","22b276b56dda46f999aec525cf70dfb1"]},"id":"7taQUL7v5Ohu","outputId":"deaf4ec5-ac17-4bf2-a292-5f260a27c847"},"outputs":[],"source":["metrics = model.eval_metrics(\n","    data=pool1,\n","    metrics=['Logloss','AUC'],\n","    ntree_start=0,\n","    ntree_end=0,\n","    eval_period=1,\n","    plot=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UD4ATFHB5Oh0","outputId":"42a593e0-3a43-4e66-d510-6311634b77e2"},"outputs":[],"source":["print('AUC values:')\n","print(np.array(metrics['AUC']))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UIVIjy4s5Oh3"},"source":["#### <b> Feature Importances </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ITnCQi5VNklo"},"source":["<b> Feature importance </b> refers to techniques that assign a score to input features based on how useful they are at predicting a target variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h62IP0nx5Oh4","outputId":"21bd1304-8cc0-4417-a760-3779b695a739"},"outputs":[],"source":["#To find feature importance\n","model.get_feature_importance(prettified=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"837_z1K8Nklp"},"source":["Scores are assigned to the input features."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kj1c82tM5OiP"},"source":["#### <b> Feature Evaluation </b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EtDVXC7IShph"},"source":["Let us perform feature evaluation using the **eval_features( )** function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWbMgw2v5OiQ","scrolled":true},"outputs":[],"source":["from catboost.eval.catboost_evaluation import *\n","learn_params = {'iterations': 20, # 2000\n","                'learning_rate': 0.5, #we set big learning_rate, because we have small iterations\n","                'random_seed': 0,\n","                'verbose': False,\n","                'loss_function' : 'Logloss',\n","                'boosting_type': 'Plain'}\n","evaluator = CatboostEvaluation('amazon/train.tsv',\n","                               fold_size=10000, #<= 50% of dataset\n","                               fold_count=20,\n","                               column_description='amazon/train.cd',\n","                               partition_random_seed=0,\n","                               #working_dir=... \n",")\n","result = evaluator.eval_features(learn_config=learn_params,\n","                                 eval_metrics=['Logloss', 'Accuracy'],\n","                                 features_to_eval=[6, 7, 8])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spKLWsNT5OiT","outputId":"cbb7fcea-a18a-4691-8f35-cef203c6f9f3"},"outputs":[],"source":["from catboost.eval.evaluation_result import *\n","logloss_result = result.get_metric_results('Logloss')\n","logloss_result.get_baseline_comparison(\n","    ScoreConfig(ScoreType.Rel, overfit_iterations_info=False)\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UcZJ4fdD5OiX"},"source":["#### <b> Saving the Model </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xglg7jcd5OiX"},"outputs":[],"source":["my_best_model = CatBoostClassifier(iterations=10)\n","my_best_model.fit(\n","    X_train, y_train,\n","    eval_set=(X_validation, y_validation),\n","    cat_features=cat_features,\n","    verbose=False\n",")\n","my_best_model.save_model('catboost_model.bin')\n","my_best_model.save_model('catboost_model.json', format='json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgMDZPUH5Oi6","outputId":"363c4aa5-05da-4016-d67f-526b25d6f73f"},"outputs":[],"source":["my_best_model.load_model('catboost_model.bin')\n","print(my_best_model.get_params())\n","print(my_best_model.random_seed_)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0aAqAHuV5Oi9"},"source":["### **Hyperparameter Tunning**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_D23SEkMNklp"},"source":["Hyperparameter tuning is the process of determining the right combination of hyperparameters that allows the model to maximize model performance. Setting the correct combination of hyperparameters is the only way to extract the maximum performance out of models."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GzuTd82i5Oi9"},"source":["#### <b> Training Speed </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["5919aaf4ce58490d8010972c0d27e2d9","76030f163f7c44199cbae12b33a3b1e3"]},"id":"U9T_sgtS5Oi9","outputId":"eb7b6801-1b60-4dc5-8320-9d35d28c1f7d"},"outputs":[],"source":["from catboost import CatBoost\n","fast_model = CatBoostClassifier(\n","    random_seed=63,\n","    iterations=150,\n","    learning_rate=0.01,\n","    boosting_type='Plain',\n","    bootstrap_type='Bernoulli',\n","    subsample=0.5,\n","    one_hot_max_size=20,\n","    rsm=0.5,\n","    leaf_estimation_iterations=5,\n","    max_ctr_complexity=1)\n","\n","fast_model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    verbose=False,\n","    plot=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dJ4Q2M3S5OjB"},"source":["#### <b> Accuracy </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["a753f2a03cd64448969399ae15bf5f8e","ef0114e102ed4ff694f884264ec7d64d"]},"id":"6AHvnAq95OjB","outputId":"50e8d209-c8dd-420f-ce47-3768cd0986a9"},"outputs":[],"source":["tunned_model = CatBoostClassifier(\n","    random_seed=63,\n","    iterations=1000,\n","    learning_rate=0.03,\n","    l2_leaf_reg=3,\n","    bagging_temperature=1,\n","    random_strength=1,\n","    one_hot_max_size=2,\n","    leaf_estimation_method='Newton'\n",")\n","tunned_model.fit(\n","    X_train, y_train,\n","    cat_features=cat_features,\n","    verbose=False,\n","    eval_set=(X_validation, y_validation),\n","    plot=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ICXlZNSk5OjE"},"source":["#### <b> Training the Model after Parameter Tuning </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNRcMeN65OjF","outputId":"fd5992b7-15e8-4219-9efc-7ca073977848"},"outputs":[],"source":["best_model = CatBoostClassifier(\n","    random_seed=63,\n","    iterations=int(tunned_model.tree_count_ * 1.2),\n",")\n","best_model.fit(\n","    X, y,\n","    cat_features=cat_features,\n","    verbose=100\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qW0i-lul5OjI"},"source":["#### <b> Calculate Prediction </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hIfCEu-b5OjJ","outputId":"35628d37-1425-4140-8ce3-3028339f7649"},"outputs":[],"source":["#Let us calculate contest predictions\n","X_test = test_df.drop('id', axis=1)\n","test_pool = Pool(data=X_test, cat_features=cat_features)\n","contest_predictions = best_model.predict_proba(test_pool)\n","print('Predictions:')\n","print(contest_predictions)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vnyu4ovB5OjL"},"source":["### <b>Voting Ensemble</b>\n","\n","Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n","\n","It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the submodels when asked to make predictions for new data.\n","\n","The predictions of the submodels can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked generalization) and is currently not provided in scikit-learn.\n","\n","You can create a voting ensemble model for classification using the **VotingClassifier** class.\n","\n","The code below provides an example of combining the predictions of logistic regression, classification, and regression trees and support vector machines together for a classification problem.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmMBPdN58cMW","outputId":"ca81f02d-ec1a-46b1-a6b0-20c5c47cd309"},"outputs":[],"source":["#Voting Ensemble for Classification\n","import pandas\n","from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import VotingClassifier\n","\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = pandas.read_csv(url, names=names)\n","\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","seed = 7\n","kfold = model_selection.KFold(n_splits=10)\n","\n","#Create the sub models\n","estimators = []\n","model1 = LogisticRegression()\n","estimators.append(('logistic', model1))\n","model2 = DecisionTreeClassifier()\n","estimators.append(('cart', model2))\n","model3 = SVC()\n","estimators.append(('svm', model3))\n","\n","#Create the ensemble model\n","ensemble = VotingClassifier(estimators)\n","results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n","print(results.mean())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3xKjDdV7WeLM"},"source":["**Note: In this lesson, we saw the use of the ensemble learning methods, and in the next lesson, we will be working on Recommender Systems.**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"x5yqWygG8lPX"},"source":["![Simplilearn_Logo](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Logo_Powered_By_Simplilearn/SL_Logo_1.png)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"7.01_Ensemble_Learning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
